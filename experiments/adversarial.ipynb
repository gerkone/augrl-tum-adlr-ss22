{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import d3rlpy\n",
    "from d3rlpy.dataset import MDPDataset\n",
    "from d3rlpy.metrics.scorer import average_value_estimation_scorer, td_error_scorer, evaluate_on_environment\n",
    "from d3rlpy.models.q_functions import QRQFunctionFactory\n",
    "from d3rlpy.models.encoders import VectorEncoderFactory\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#libraries for PSO optimization - to discuss with tutor\n",
    "import pyswarms as ps\n",
    "from pyswarms.utils.functions import single_obj as fx\n",
    "\n",
    "import random\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading cartpole.pkl into d3rlpy_data/cartpole_replay_v1.1.0.h5...\n",
      "2022-05-30 15:19.53 [debug    ] RoundIterator is selected.\n",
      "2022-05-30 15:19.53 [info     ] Directory is created at d3rlpy_logs/DiscreteCQL_20220530151953\n",
      "2022-05-30 15:19.53 [debug    ] Building models...\n",
      "2022-05-30 15:19.53 [debug    ] Models have been built.\n",
      "2022-05-30 15:19.53 [info     ] Parameters are saved to d3rlpy_logs/DiscreteCQL_20220530151953/params.json params={'action_scaler': None, 'alpha': 1.0, 'batch_size': 32, 'encoder_factory': {'type': 'vector', 'params': {'hidden_units': [300, 400], 'activation': 'tanh', 'use_batch_norm': True, 'dropout_rate': 0.3, 'use_dense': True}}, 'gamma': 0.99, 'generated_maxlen': 100000, 'learning_rate': 6.25e-05, 'n_critics': 1, 'n_frames': 1, 'n_steps': 1, 'optim_factory': {'optim_cls': 'Adam', 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}, 'q_func_factory': {'type': 'qr', 'params': {'share_encoder': False, 'n_quantiles': 32}}, 'real_ratio': 1.0, 'reward_scaler': None, 'scaler': None, 'target_update_interval': 8000, 'use_gpu': None, 'algorithm': 'DiscreteCQL', 'observation_shape': (4,), 'action_size': 2}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e0f9d0284f5441eaf1890621eed5515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/3:   0%|          | 0/2335 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-30 15:20.17 [info     ] DiscreteCQL_20220530151953: epoch=1 step=2335 epoch=1 metrics={'time_sample_batch': 0.00011720708401861743, 'time_algorithm_update': 0.00864649993193992, 'loss': 7.219594479832455, 'time_step': 0.008873230180597408, 'td_error': 1.0362029242990671, 'value_scale': 0.87362166705126, 'environment': 200.0} step=2335\n",
      "2022-05-30 15:20.17 [info     ] Model parameters are saved to d3rlpy_logs/DiscreteCQL_20220530151953/model_2335.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "275606efa5d54b7192de922d9f872778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/3:   0%|          | 0/2335 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-30 15:20.41 [info     ] DiscreteCQL_20220530151953: epoch=2 step=4670 epoch=2 metrics={'time_sample_batch': 0.00010366470451273296, 'time_algorithm_update': 0.008969639608640487, 'loss': 3.793903584041228, 'time_step': 0.009160214675315187, 'td_error': 1.0150854121632007, 'value_scale': 0.9647937832399531, 'environment': 200.0} step=4670\n",
      "2022-05-30 15:20.41 [info     ] Model parameters are saved to d3rlpy_logs/DiscreteCQL_20220530151953/model_4670.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6783e02398d94a9a90746dfcfa020124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/3:   0%|          | 0/2335 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-30 15:21.10 [info     ] DiscreteCQL_20220530151953: epoch=3 step=7005 epoch=3 metrics={'time_sample_batch': 0.00010660343047638266, 'time_algorithm_update': 0.009484074610969494, 'loss': 3.47637835130957, 'time_step': 0.009681531142371625, 'td_error': 1.0178133726227743, 'value_scale': 0.9812350331680056, 'environment': 200.0} step=7005\n",
      "2022-05-30 15:21.10 [info     ] Model parameters are saved to d3rlpy_logs/DiscreteCQL_20220530151953/model_7005.pt\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = False\n",
    "EPOCHS = 3\n",
    "\n",
    "#TODO - try with Mujoco environments. Currently Mujoco not working on my mac.\n",
    "dataset, env = d3rlpy.datasets.get_cartpole()\n",
    "\n",
    "#we setup a custom encoder. By tuning the parameters we could search for a better model (maybe implement random search)\n",
    "encoder = VectorEncoderFactory(hidden_units=[300, 400], use_batch_norm = True, activation='tanh', dropout_rate=0.3, use_dense=True)\n",
    "\n",
    "agent = d3rlpy.algos.DiscreteCQL(q_func_factory='qr', encoder_factory=encoder, use_gpu = USE_GPU)\n",
    "\n",
    "train_episodes, test_episodes = train_test_split(dataset)\n",
    "\n",
    "#training returns list of result tuples (epoch, metrics) per epoch.\n",
    "data = agent.fit(\n",
    "    dataset = train_episodes,\n",
    "    eval_episodes = test_episodes,\n",
    "    n_epochs = EPOCHS,\n",
    "    scorers = {\n",
    "        'td_error': td_error_scorer,   #Returns average TD error (how Q function overfits to training set, the smaller the better).\n",
    "        'value_scale': average_value_estimation_scorer, # If too large, the Q functions overestimate action-values\n",
    "        'environment': evaluate_on_environment(env)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "SIZE_AUG = 100\n",
    "ITER = 100\n",
    "eps = 0.1\n",
    "\n",
    "\"\"\"\n",
    "Tentative of adversarial state training implementation\n",
    "Not being able at the moment to access the gradient of the value function, I considered a simplified version of the problem.\n",
    "We perform a random search over the value function space, and we keep the worst observation to use for the new dataset.\n",
    "\n",
    "We collect a sample of observations (size: SIZE_AUG), and we compute the state value, by using Montecarlo integration over the action space. \n",
    "    value = 1/N * [Q(state, action_1) + Q(state, action_2) + ... + Q(state, action_3)]\n",
    "We iterate over all the observation: we perturb the observation state by adding gaussian noise and we compute the value function, as described above.\n",
    "Finally we compare the obtained value with the worst value (initially set to the optimal state value)\n",
    "We do this for ITER number of iterations.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#fetch #SIZE_AUG observations (np array) from dataset\n",
    "observations = dataset.observations\n",
    "\n",
    "#find optimal Q-value for each observation (action is sampled from policy)\n",
    "#in the continuous case we could sample N actions, and average (kind of Montecarlo approx over the actions)\n",
    "values = 1/2 * (agent.predict_value(observations[:SIZE_AUG,:], np.zeros(SIZE_AUG) + agent.predict_value(observations[:SIZE_AUG,:], np.ones(SIZE_AUG))))\n",
    "\n",
    "for  i, obs in enumerate(observations[:SIZE_AUG,:]):\n",
    "    print(i)\n",
    "    worst_value = values[i]\n",
    "    for x in range(ITER):\n",
    "        #sample a state in the epsilon ball, and an action.\n",
    "        perturbed_state = (obs + np.random.normal(loc = 0, scale = eps, size=obs.shape)).reshape(1,-1)\n",
    "        \n",
    "        #compute the state value for the \n",
    "        perturbed_value = 1/2 * (agent.predict_value(perturbed_state, np.array([0])) + agent.predict_value(perturbed_state, np.array([1])))\n",
    "        if perturbed_value < worst_value:\n",
    "            worst_value = perturbed_value\n",
    "            worst_obs = perturbed_state\n",
    "    observations[i] = worst_obs\n",
    "\n",
    "\n",
    "dataset_aug = MDPDataset(\n",
    "    observations, \n",
    "    dataset.actions, \n",
    "    dataset.rewards, \n",
    "    dataset.terminals\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-30 16:30.12 [debug    ] RoundIterator is selected.\n",
      "2022-05-30 16:30.12 [info     ] Directory is created at d3rlpy_logs/DiscreteCQL_20220530163012\n",
      "2022-05-30 16:30.12 [warning  ] Skip building models since they're already built.\n",
      "2022-05-30 16:30.12 [info     ] Parameters are saved to d3rlpy_logs/DiscreteCQL_20220530163012/params.json params={'action_scaler': None, 'alpha': 1.0, 'batch_size': 32, 'encoder_factory': {'type': 'vector', 'params': {'hidden_units': [300, 400], 'activation': 'tanh', 'use_batch_norm': True, 'dropout_rate': 0.3, 'use_dense': True}}, 'gamma': 0.99, 'generated_maxlen': 100000, 'learning_rate': 6.25e-05, 'n_critics': 1, 'n_frames': 1, 'n_steps': 1, 'optim_factory': {'optim_cls': 'Adam', 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}, 'q_func_factory': {'type': 'qr', 'params': {'share_encoder': False, 'n_quantiles': 32}}, 'real_ratio': 1.0, 'reward_scaler': None, 'scaler': None, 'target_update_interval': 8000, 'use_gpu': None, 'algorithm': 'DiscreteCQL', 'observation_shape': (4,), 'action_size': 2}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f754f2386e6547feae60aa551f7a2966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/3:   0%|          | 0/2469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-30 16:30.37 [info     ] DiscreteCQL_20220530163012: epoch=1 step=2469 epoch=1 metrics={'time_sample_batch': 9.79771928876071e-05, 'time_algorithm_update': 0.008146137494025032, 'loss': 3.1916634771880856, 'time_step': 0.008330075931046935, 'td_error': 1.006930780789414, 'value_scale': 1.9722501374804455, 'environment': 200.0} step=2469\n",
      "2022-05-30 16:30.37 [info     ] Model parameters are saved to d3rlpy_logs/DiscreteCQL_20220530163012/model_2469.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c3cf6c8fcd43bdbf9c30d5acb72ca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/3:   0%|          | 0/2469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-30 16:31.02 [info     ] DiscreteCQL_20220530163012: epoch=2 step=4938 epoch=2 metrics={'time_sample_batch': 0.00010064363769308764, 'time_algorithm_update': 0.00872278667355704, 'loss': 2.8635760621533985, 'time_step': 0.008908869815577664, 'td_error': 0.9993534448050594, 'value_scale': 1.954566751837541, 'environment': 200.0} step=4938\n",
      "2022-05-30 16:31.02 [info     ] Model parameters are saved to d3rlpy_logs/DiscreteCQL_20220530163012/model_4938.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c01d12f1e9c4c6aa66879fa1f2ad377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/3:   0%|          | 0/2469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-30 16:31.26 [info     ] DiscreteCQL_20220530163012: epoch=3 step=7407 epoch=3 metrics={'time_sample_batch': 9.559435900428991e-05, 'time_algorithm_update': 0.008054034146957429, 'loss': 2.7956504975315433, 'time_step': 0.00823249349520727, 'td_error': 0.9943116620080363, 'value_scale': 1.9583272648381285, 'environment': 200.0} step=7407\n",
      "2022-05-30 16:31.26 [info     ] Model parameters are saved to d3rlpy_logs/DiscreteCQL_20220530163012/model_7407.pt\n"
     ]
    }
   ],
   "source": [
    "encoder_aug = VectorEncoderFactory(hidden_units=[300, 400], use_batch_norm = True, activation='tanh', dropout_rate=0.3, use_dense=True)\n",
    "\n",
    "agent_aug = d3rlpy.algos.DiscreteCQL(q_func_factory='qr', encoder_factory=encoder_aug, use_gpu = USE_GPU)\n",
    "\n",
    "train_episodes, test_episodes = train_test_split(dataset_aug)\n",
    "\n",
    "#training returns list of result tuples (epoch, metrics) per epoch.\n",
    "data = agent.fit(\n",
    "    dataset = train_episodes,\n",
    "    eval_episodes = test_episodes,\n",
    "    n_epochs = EPOCHS,\n",
    "    scorers = {\n",
    "        'td_error': td_error_scorer,   #Returns average TD error (how Q function overfits to training set, the smaller the better).\n",
    "        'value_scale': average_value_estimation_scorer, # If too large, the Q functions overestimate action-values\n",
    "        'environment': evaluate_on_environment(env)\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
